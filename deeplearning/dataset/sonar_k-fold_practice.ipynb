{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0fa0d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "65187728",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2a9d596c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.1609</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.2238</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.2999</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>0.4797</td>\n",
       "      <td>0.5783</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.4328</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.6711</td>\n",
       "      <td>0.6415</td>\n",
       "      <td>0.7104</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>0.7547</td>\n",
       "      <td>0.8537</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.6097</td>\n",
       "      <td>0.4943</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.2834</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.2641</td>\n",
       "      <td>0.1386</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.4918</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.7464</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>0.8024</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.5212</td>\n",
       "      <td>0.4052</td>\n",
       "      <td>0.3957</td>\n",
       "      <td>0.3914</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.3271</td>\n",
       "      <td>0.2767</td>\n",
       "      <td>0.4423</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.3788</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>0.1984</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>0.4182</td>\n",
       "      <td>0.3835</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.0621</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.7060</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>0.6479</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.7974</td>\n",
       "      <td>0.6737</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.3648</td>\n",
       "      <td>0.5331</td>\n",
       "      <td>0.2413</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>0.8533</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.8514</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>0.4232</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>0.6116</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>0.2129</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.1348</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.1992</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.1729</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>0.2281</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.3973</td>\n",
       "      <td>0.2741</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.5334</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>0.6260</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>0.6120</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>0.3953</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>0.5408</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.6121</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>0.3202</td>\n",
       "      <td>0.4295</td>\n",
       "      <td>0.3654</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.3952</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.4135</td>\n",
       "      <td>0.4528</td>\n",
       "      <td>0.5326</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6193</td>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.4292</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.6995</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.4724</td>\n",
       "      <td>0.5103</td>\n",
       "      <td>0.5459</td>\n",
       "      <td>0.2881</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1951</td>\n",
       "      <td>0.4181</td>\n",
       "      <td>0.4604</td>\n",
       "      <td>0.3217</td>\n",
       "      <td>0.2828</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.1979</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>0.3108</td>\n",
       "      <td>0.2933</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>0.0994</td>\n",
       "      <td>0.1801</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2034</td>\n",
       "      <td>0.1740</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>0.6879</td>\n",
       "      <td>0.8120</td>\n",
       "      <td>0.8453</td>\n",
       "      <td>0.8919</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8104</td>\n",
       "      <td>0.6199</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>0.4160</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0969</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.1676</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>0.1977</td>\n",
       "      <td>0.1339</td>\n",
       "      <td>0.0902</td>\n",
       "      <td>0.1085</td>\n",
       "      <td>0.1521</td>\n",
       "      <td>0.1363</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>0.3085</td>\n",
       "      <td>0.3425</td>\n",
       "      <td>0.2990</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>0.1534</td>\n",
       "      <td>0.1901</td>\n",
       "      <td>0.2429</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.3272</td>\n",
       "      <td>0.5949</td>\n",
       "      <td>0.8302</td>\n",
       "      <td>0.9045</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9092</td>\n",
       "      <td>0.7412</td>\n",
       "      <td>0.7691</td>\n",
       "      <td>0.7117</td>\n",
       "      <td>0.5304</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>0.1297</td>\n",
       "      <td>0.1159</td>\n",
       "      <td>0.1226</td>\n",
       "      <td>0.1768</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.1149</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0790</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>0.2716</td>\n",
       "      <td>0.2374</td>\n",
       "      <td>0.1878</td>\n",
       "      <td>0.0983</td>\n",
       "      <td>0.0683</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.1723</td>\n",
       "      <td>0.2339</td>\n",
       "      <td>0.1962</td>\n",
       "      <td>0.1395</td>\n",
       "      <td>0.3164</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>0.7631</td>\n",
       "      <td>0.8473</td>\n",
       "      <td>0.9424</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9699</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8630</td>\n",
       "      <td>0.6979</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.7305</td>\n",
       "      <td>0.5197</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1446</td>\n",
       "      <td>0.1066</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.1929</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>0.0910</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>0.1059</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>0.0535</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.2898</td>\n",
       "      <td>0.2812</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.0673</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.2070</td>\n",
       "      <td>0.2645</td>\n",
       "      <td>0.2828</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.5685</td>\n",
       "      <td>0.6990</td>\n",
       "      <td>0.7246</td>\n",
       "      <td>0.7622</td>\n",
       "      <td>0.9242</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.8297</td>\n",
       "      <td>0.7032</td>\n",
       "      <td>0.7141</td>\n",
       "      <td>0.6893</td>\n",
       "      <td>0.4961</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.0969</td>\n",
       "      <td>0.0776</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.1572</td>\n",
       "      <td>0.1823</td>\n",
       "      <td>0.1349</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.1552</td>\n",
       "      <td>0.1548</td>\n",
       "      <td>0.1319</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>0.2442</td>\n",
       "      <td>0.1665</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.1302</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>0.2177</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.4552</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.7397</td>\n",
       "      <td>0.8062</td>\n",
       "      <td>0.8837</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.7603</td>\n",
       "      <td>0.7123</td>\n",
       "      <td>0.8358</td>\n",
       "      <td>0.7622</td>\n",
       "      <td>0.4567</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.1549</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.1869</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.1713</td>\n",
       "      <td>0.0959</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.0847</td>\n",
       "      <td>0.2076</td>\n",
       "      <td>0.2505</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.1439</td>\n",
       "      <td>0.1470</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6   ...      54      55      56      57      58      59  60\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  ...  0.0072  0.0167  0.0180  0.0084  0.0090  0.0032   R\n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  ...  0.0094  0.0191  0.0140  0.0049  0.0052  0.0044   R\n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  ...  0.0180  0.0244  0.0316  0.0164  0.0095  0.0078   R\n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  ...  0.0085  0.0073  0.0050  0.0044  0.0040  0.0117   R\n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  ...  0.0110  0.0015  0.0072  0.0048  0.0107  0.0094   R\n",
       "..      ...     ...     ...     ...     ...     ...     ...  ...     ...     ...     ...     ...     ...     ...  ..\n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  ...  0.0033  0.0101  0.0065  0.0115  0.0193  0.0157   M\n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  ...  0.0063  0.0063  0.0034  0.0032  0.0062  0.0067   M\n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  ...  0.0062  0.0089  0.0140  0.0138  0.0077  0.0031   M\n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  ...  0.0036  0.0035  0.0034  0.0079  0.0036  0.0048   M\n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  ...  0.0039  0.0061  0.0040  0.0036  0.0061  0.0115   M\n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./sonar.csv', header = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ef1d1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02, 0.0371, 0.0428, ..., 0.009, 0.0032, 'R'],\n",
       "       [0.0453, 0.0523, 0.0843, ..., 0.0052, 0.0044, 'R'],\n",
       "       [0.0262, 0.0582, 0.1099, ..., 0.0095, 0.0078, 'R'],\n",
       "       ...,\n",
       "       [0.0522, 0.0437, 0.018, ..., 0.0077, 0.0031, 'M'],\n",
       "       [0.0303, 0.0353, 0.049, ..., 0.0036, 0.0048, 'M'],\n",
       "       [0.026, 0.0363, 0.0136, ..., 0.0061, 0.0115, 'M']], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cfb6400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.1609</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.2238</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.2999</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>0.4797</td>\n",
       "      <td>0.5783</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.4328</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.6711</td>\n",
       "      <td>0.6415</td>\n",
       "      <td>0.7104</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>0.7547</td>\n",
       "      <td>0.8537</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.6097</td>\n",
       "      <td>0.4943</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.2834</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.2641</td>\n",
       "      <td>0.1386</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.4918</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.7464</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>0.8024</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.5212</td>\n",
       "      <td>0.4052</td>\n",
       "      <td>0.3957</td>\n",
       "      <td>0.3914</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.3271</td>\n",
       "      <td>0.2767</td>\n",
       "      <td>0.4423</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.3788</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>0.1984</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>0.4182</td>\n",
       "      <td>0.3835</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.0621</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.7060</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>0.6479</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.7974</td>\n",
       "      <td>0.6737</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.3648</td>\n",
       "      <td>0.5331</td>\n",
       "      <td>0.2413</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>0.8533</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.8514</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>0.4232</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>0.6116</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>0.2129</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.1348</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.1992</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.1729</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>0.2281</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.3973</td>\n",
       "      <td>0.2741</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.5334</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>0.6260</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>0.6120</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>0.3953</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>0.5408</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.6121</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>0.3202</td>\n",
       "      <td>0.4295</td>\n",
       "      <td>0.3654</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.3952</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.4135</td>\n",
       "      <td>0.4528</td>\n",
       "      <td>0.5326</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6193</td>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.4292</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.6995</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.4724</td>\n",
       "      <td>0.5103</td>\n",
       "      <td>0.5459</td>\n",
       "      <td>0.2881</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1951</td>\n",
       "      <td>0.4181</td>\n",
       "      <td>0.4604</td>\n",
       "      <td>0.3217</td>\n",
       "      <td>0.2828</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.1979</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>0.3108</td>\n",
       "      <td>0.2933</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>0.0994</td>\n",
       "      <td>0.1801</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2034</td>\n",
       "      <td>0.1740</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>0.6879</td>\n",
       "      <td>0.8120</td>\n",
       "      <td>0.8453</td>\n",
       "      <td>0.8919</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8104</td>\n",
       "      <td>0.6199</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>0.4160</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0969</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.1676</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>0.1977</td>\n",
       "      <td>0.1339</td>\n",
       "      <td>0.0902</td>\n",
       "      <td>0.1085</td>\n",
       "      <td>0.1521</td>\n",
       "      <td>0.1363</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>0.3085</td>\n",
       "      <td>0.3425</td>\n",
       "      <td>0.2990</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>0.1534</td>\n",
       "      <td>0.1901</td>\n",
       "      <td>0.2429</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.3272</td>\n",
       "      <td>0.5949</td>\n",
       "      <td>0.8302</td>\n",
       "      <td>0.9045</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9092</td>\n",
       "      <td>0.7412</td>\n",
       "      <td>0.7691</td>\n",
       "      <td>0.7117</td>\n",
       "      <td>0.5304</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>0.1297</td>\n",
       "      <td>0.1159</td>\n",
       "      <td>0.1226</td>\n",
       "      <td>0.1768</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.1149</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0790</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>0.2716</td>\n",
       "      <td>0.2374</td>\n",
       "      <td>0.1878</td>\n",
       "      <td>0.0983</td>\n",
       "      <td>0.0683</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.1723</td>\n",
       "      <td>0.2339</td>\n",
       "      <td>0.1962</td>\n",
       "      <td>0.1395</td>\n",
       "      <td>0.3164</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>0.7631</td>\n",
       "      <td>0.8473</td>\n",
       "      <td>0.9424</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9699</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8630</td>\n",
       "      <td>0.6979</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.7305</td>\n",
       "      <td>0.5197</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1446</td>\n",
       "      <td>0.1066</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.1929</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>0.0910</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>0.1059</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>0.0535</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.0490</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.1123</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.2898</td>\n",
       "      <td>0.2812</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.0673</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.2070</td>\n",
       "      <td>0.2645</td>\n",
       "      <td>0.2828</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.5685</td>\n",
       "      <td>0.6990</td>\n",
       "      <td>0.7246</td>\n",
       "      <td>0.7622</td>\n",
       "      <td>0.9242</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>0.8297</td>\n",
       "      <td>0.7032</td>\n",
       "      <td>0.7141</td>\n",
       "      <td>0.6893</td>\n",
       "      <td>0.4961</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.0969</td>\n",
       "      <td>0.0776</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.1572</td>\n",
       "      <td>0.1823</td>\n",
       "      <td>0.1349</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.1552</td>\n",
       "      <td>0.1548</td>\n",
       "      <td>0.1319</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.0136</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.1843</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>0.2442</td>\n",
       "      <td>0.1665</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.1302</td>\n",
       "      <td>0.1708</td>\n",
       "      <td>0.2177</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>0.4552</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.7397</td>\n",
       "      <td>0.8062</td>\n",
       "      <td>0.8837</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>0.7603</td>\n",
       "      <td>0.7123</td>\n",
       "      <td>0.8358</td>\n",
       "      <td>0.7622</td>\n",
       "      <td>0.4567</td>\n",
       "      <td>0.1715</td>\n",
       "      <td>0.1549</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>0.1869</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.1713</td>\n",
       "      <td>0.0959</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.0847</td>\n",
       "      <td>0.2076</td>\n",
       "      <td>0.2505</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.1439</td>\n",
       "      <td>0.1470</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6   ...      54      55      56      57      58      59  60\n",
       "0    0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  ...  0.0072  0.0167  0.0180  0.0084  0.0090  0.0032   R\n",
       "1    0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  ...  0.0094  0.0191  0.0140  0.0049  0.0052  0.0044   R\n",
       "2    0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  ...  0.0180  0.0244  0.0316  0.0164  0.0095  0.0078   R\n",
       "3    0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  ...  0.0085  0.0073  0.0050  0.0044  0.0040  0.0117   R\n",
       "4    0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  ...  0.0110  0.0015  0.0072  0.0048  0.0107  0.0094   R\n",
       "..      ...     ...     ...     ...     ...     ...     ...  ...     ...     ...     ...     ...     ...     ...  ..\n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  ...  0.0033  0.0101  0.0065  0.0115  0.0193  0.0157   M\n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  ...  0.0063  0.0063  0.0034  0.0032  0.0062  0.0067   M\n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  ...  0.0062  0.0089  0.0140  0.0138  0.0077  0.0031   M\n",
       "206  0.0303  0.0353  0.0490  0.0608  0.0167  0.1354  0.1465  ...  0.0036  0.0035  0.0034  0.0079  0.0036  0.0048   M\n",
       "207  0.0260  0.0363  0.0136  0.0272  0.0214  0.0338  0.0655  ...  0.0039  0.0061  0.0040  0.0036  0.0061  0.0115   M\n",
       "\n",
       "[208 rows x 61 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e6a3a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df # dataset = dfë¡œ í• ê²½ìš° ë°ì´í„°í”„ë ˆìž„ì„ ê·¸ëŒ€ë¡œ ì“°ê¸°ë•Œë¬¸ì— ilocë‚˜ locì¨ì„œ ìŠ¬ë¼ì´ì‹±í•´ì¤„ê²ƒ.\n",
    "X = dataset.iloc[:,0:60].astype(float)\n",
    "y = dataset.iloc[:,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0896bed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function ndarray.astype>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.astype\n",
    "df.values.astype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "624a81a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = df.values # dataset = df.valuesë¡œ í• ê²½ìš° arrayí˜•ì‹ìœ¼ë¡œ ë°”ë€Œì–´ì§. ê·¸ëƒ¥ ë°”ë¡œ ìŠ¬ë¼ì´ì‹± í•´ì£¼ë©´ ë¨.\n",
    "X = dataset[:, 0:60].astype(float)\n",
    "y = dataset[:, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ce790389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ì½”ë”©í•˜ê¸°\n",
    "e = LabelEncoder()\n",
    "e.fit(y)\n",
    "Y = e.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c4c1c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10ê°œ íŒŒì¼ë¡œ ìª¼ê°œê¸°\n",
    "n_fold = 10\n",
    "skf = StratifiedKFold(n_splits = n_fold, shuffle = True, random_state=seed)\n",
    "\n",
    "accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bd182ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2507 - accuracy: 0.5591\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2407 - accuracy: 0.5403\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2277 - accuracy: 0.6346\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.1996 - accuracy: 0.7417\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.1999 - accuracy: 0.6834\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 913us/step - loss: 0.1809 - accuracy: 0.7605\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.1796 - accuracy: 0.7565\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.1684 - accuracy: 0.7918\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.1579 - accuracy: 0.8158\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.1457 - accuracy: 0.7904\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.1497 - accuracy: 0.8091\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.1476 - accuracy: 0.7931\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.1462 - accuracy: 0.8311\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.1372 - accuracy: 0.7794\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.1302 - accuracy: 0.8252\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 947us/step - loss: 0.1264 - accuracy: 0.8610\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.1187 - accuracy: 0.8833\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 955us/step - loss: 0.1142 - accuracy: 0.8870\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.1229 - accuracy: 0.8347\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.1273 - accuracy: 0.7861\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 913us/step - loss: 0.1438 - accuracy: 0.7971\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.0983 - accuracy: 0.8858\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.1242 - accuracy: 0.8192\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.1063 - accuracy: 0.8598\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.1062 - accuracy: 0.8573\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.0909 - accuracy: 0.8987\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0984 - accuracy: 0.8983\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.1126 - accuracy: 0.8763\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.0877 - accuracy: 0.8999\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.1091 - accuracy: 0.8336\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0984 - accuracy: 0.8700\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.0922 - accuracy: 0.8787\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.0830 - accuracy: 0.9061\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0950 - accuracy: 0.8902\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 896us/step - loss: 0.0799 - accuracy: 0.9067\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.0919 - accuracy: 0.8846\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.0665 - accuracy: 0.9187\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.0808 - accuracy: 0.8964\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0787 - accuracy: 0.8760\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0764 - accuracy: 0.9116\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.0660 - accuracy: 0.9297\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.0714 - accuracy: 0.9298\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.0898 - accuracy: 0.8925\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.0712 - accuracy: 0.9044\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.0649 - accuracy: 0.9392\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.0693 - accuracy: 0.9357\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.0717 - accuracy: 0.9308\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.0576 - accuracy: 0.9580\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.0678 - accuracy: 0.8861\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.0590 - accuracy: 0.9300\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.0516 - accuracy: 0.9483\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0560 - accuracy: 0.9332\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0482 - accuracy: 0.9607\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0630 - accuracy: 0.9163\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0409 - accuracy: 0.9580\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.0615 - accuracy: 0.9136\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0447 - accuracy: 0.9621\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.0582 - accuracy: 0.9459\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.0460 - accuracy: 0.9464\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.0457 - accuracy: 0.9616\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.0458 - accuracy: 0.9706\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.0362 - accuracy: 0.9809\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.0331 - accuracy: 0.9771\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.0369 - accuracy: 0.9526\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.0457 - accuracy: 0.9388\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.0390 - accuracy: 0.9648\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.0321 - accuracy: 0.9710\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.0310 - accuracy: 0.9752\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.0332 - accuracy: 0.9802\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.0354 - accuracy: 0.9557\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0367 - accuracy: 0.9631\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.0292 - accuracy: 0.9853\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.0264 - accuracy: 0.9925\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.0286 - accuracy: 0.9734\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.0451 - accuracy: 0.9632\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.0363 - accuracy: 0.9671\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.0284 - accuracy: 0.9752\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0338 - accuracy: 0.9849\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.0256 - accuracy: 0.9938\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.0280 - accuracy: 0.9769\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 885us/step - loss: 0.0300 - accuracy: 0.9878\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.0264 - accuracy: 0.9790\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.0242 - accuracy: 0.9838\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.0205 - accuracy: 0.9961\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.0182 - accuracy: 0.9933\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.0178 - accuracy: 0.9861\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.0283 - accuracy: 0.9780\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.0281 - accuracy: 0.9757\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.0192 - accuracy: 0.9958\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.0200 - accuracy: 0.9791\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.0147 - accuracy: 0.9991\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0144 - accuracy: 0.9963\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0162 - accuracy: 0.9952\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.0162 - accuracy: 0.9917\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0134 - accuracy: 0.9981\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.0174 - accuracy: 0.9924\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.0143 - accuracy: 0.9973\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.0206 - accuracy: 0.9791\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.0230 - accuracy: 0.9830\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.0202 - accuracy: 0.9782\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000271F8D4D820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.2674 - accuracy: 0.7143\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2543 - accuracy: 0.3927\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2434 - accuracy: 0.5997\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2403 - accuracy: 0.5369\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2380 - accuracy: 0.5723\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2244 - accuracy: 0.5879\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.2226 - accuracy: 0.7310\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2092 - accuracy: 0.6670\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.1907 - accuracy: 0.8042\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1784 - accuracy: 0.8362\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1683 - accuracy: 0.8173\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.1705 - accuracy: 0.7907\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.1634 - accuracy: 0.8006\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.1670 - accuracy: 0.7781\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.1566 - accuracy: 0.7414\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1572 - accuracy: 0.7962\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1513 - accuracy: 0.8047\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.1386 - accuracy: 0.8036\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1258 - accuracy: 0.8450\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.1409 - accuracy: 0.7748\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1431 - accuracy: 0.7966\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.1340 - accuracy: 0.8075\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1282 - accuracy: 0.8201\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.1303 - accuracy: 0.8172\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1164 - accuracy: 0.8260\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.1329 - accuracy: 0.8123\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1053 - accuracy: 0.8535\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1158 - accuracy: 0.8708\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1023 - accuracy: 0.8778\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1041 - accuracy: 0.8404\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.1251 - accuracy: 0.8135\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.1015 - accuracy: 0.8971\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.1053 - accuracy: 0.8737\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.1041 - accuracy: 0.8583\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1064 - accuracy: 0.8720\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.1033 - accuracy: 0.8335\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.1046 - accuracy: 0.8661\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.0957 - accuracy: 0.8806\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1018 - accuracy: 0.8610\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.0868 - accuracy: 0.9128\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 908us/step - loss: 0.0924 - accuracy: 0.8918\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0879 - accuracy: 0.8833\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0778 - accuracy: 0.9000\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.0927 - accuracy: 0.9019\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.0759 - accuracy: 0.8967\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.0842 - accuracy: 0.9063\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.0928 - accuracy: 0.8930\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.0795 - accuracy: 0.9091\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.0655 - accuracy: 0.9612\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.0996 - accuracy: 0.9004\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0797 - accuracy: 0.8801\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.0814 - accuracy: 0.9078\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.0698 - accuracy: 0.9331\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 838us/step - loss: 0.0748 - accuracy: 0.9185\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.0627 - accuracy: 0.9534\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.0640 - accuracy: 0.9324\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.0721 - accuracy: 0.9394\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.0639 - accuracy: 0.9461\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.0775 - accuracy: 0.9135\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 1.00 - 0s 858us/step - loss: 0.0641 - accuracy: 0.9399\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.0601 - accuracy: 0.9611\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.0561 - accuracy: 0.9637\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.0688 - accuracy: 0.9221\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0566 - accuracy: 0.9369\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.0631 - accuracy: 0.9141\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.0567 - accuracy: 0.9182\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.0541 - accuracy: 0.9492\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0518 - accuracy: 0.9597\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.0502 - accuracy: 0.9449\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.0443 - accuracy: 0.9621\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0616 - accuracy: 0.9175\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.0526 - accuracy: 0.9482\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.0544 - accuracy: 0.9122\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.0391 - accuracy: 0.9670\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.0372 - accuracy: 0.9678\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.0475 - accuracy: 0.9548\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.0404 - accuracy: 0.9572\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.0370 - accuracy: 0.9759\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.0411 - accuracy: 0.9771\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.0364 - accuracy: 0.9863\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.0317 - accuracy: 0.9795\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.0364 - accuracy: 0.9861\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.0325 - accuracy: 0.9870\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.0259 - accuracy: 0.9943\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.0345 - accuracy: 0.9878\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.0270 - accuracy: 0.9975\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.0284 - accuracy: 0.9941\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.0327 - accuracy: 0.9751\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.0241 - accuracy: 0.9918\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.0355 - accuracy: 0.9585\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.0177 - accuracy: 0.9820\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0203 - accuracy: 0.9859\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.0173 - accuracy: 0.9889\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0230 - accuracy: 0.9859\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.0232 - accuracy: 0.9990\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.0145 - accuracy: 0.9993\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.0173 - accuracy: 0.9985\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 1.00 - 0s 843us/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.0138 - accuracy: 0.9973\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000271F8C33820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1462 - accuracy: 0.8571\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2443 - accuracy: 0.5452\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2356 - accuracy: 0.5657\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2280 - accuracy: 0.6787\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2161 - accuracy: 0.6588\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2044 - accuracy: 0.6758\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2052 - accuracy: 0.7534\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.1917 - accuracy: 0.7315\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1858 - accuracy: 0.7363\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.1852 - accuracy: 0.7391\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1670 - accuracy: 0.7800\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.1673 - accuracy: 0.7664\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.1558 - accuracy: 0.8209\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1629 - accuracy: 0.7661\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1530 - accuracy: 0.7421\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.80 - 0s 833us/step - loss: 0.1453 - accuracy: 0.7789\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.1608 - accuracy: 0.7820\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1277 - accuracy: 0.8458\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.1238 - accuracy: 0.8415\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1353 - accuracy: 0.8090\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1353 - accuracy: 0.8182\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1501 - accuracy: 0.7737\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1199 - accuracy: 0.8591\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1126 - accuracy: 0.8533\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 814us/step - loss: 0.1045 - accuracy: 0.8751\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.0995 - accuracy: 0.8978\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.1059 - accuracy: 0.8984\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.1303 - accuracy: 1.00 - 0s 844us/step - loss: 0.0989 - accuracy: 0.8888\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.1024 - accuracy: 0.8841\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1006 - accuracy: 0.8879\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.0957 - accuracy: 0.8726\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.0930 - accuracy: 0.8751\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.0971 - accuracy: 0.8979\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0885 - accuracy: 0.8911\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.0863 - accuracy: 0.9180\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.0787 - accuracy: 0.9290\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.0863 - accuracy: 0.8977\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0713 - accuracy: 0.9249\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1163 - accuracy: 0.8212\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.0732 - accuracy: 0.9391\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0732 - accuracy: 0.9090\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.0666 - accuracy: 0.9294\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.0728 - accuracy: 0.8997\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1013 - accuracy: 0.8685\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.0692 - accuracy: 0.9195\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0725 - accuracy: 0.9484\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.0694 - accuracy: 0.9163\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.0690 - accuracy: 0.9626\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.0599 - accuracy: 0.9223\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.0766 - accuracy: 0.9252\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.0446 - accuracy: 0.9501\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.0642 - accuracy: 0.9420\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.0614 - accuracy: 0.9423\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.0512 - accuracy: 0.9458\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.0507 - accuracy: 0.9479\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.0333 - accuracy: 0.9842\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0569 - accuracy: 0.9328\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0374 - accuracy: 0.9701\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.0469 - accuracy: 0.9668\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0360 - accuracy: 0.9796\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.0479 - accuracy: 0.9575\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.0357 - accuracy: 0.9780\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.0307 - accuracy: 0.9862\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.0454 - accuracy: 0.9578\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.0368 - accuracy: 0.9712\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.0458 - accuracy: 0.9605\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.0324 - accuracy: 0.9772\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0243 - accuracy: 0.9848\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.0389 - accuracy: 0.9665\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.0298 - accuracy: 0.9744\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.0491 - accuracy: 0.9582\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.0260 - accuracy: 0.9759\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.0354 - accuracy: 0.9691\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.0305 - accuracy: 0.9718\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.0228 - accuracy: 0.9827\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.0277 - accuracy: 0.9702\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.0292 - accuracy: 0.9604\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.0255 - accuracy: 0.9833\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.0251 - accuracy: 0.9861\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0281 - accuracy: 0.9759\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.0275 - accuracy: 0.9725\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0222 - accuracy: 0.9821\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.0326 - accuracy: 0.9737\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.0352 - accuracy: 0.9671\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.0193 - accuracy: 0.9869\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0180 - accuracy: 0.9867\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.0193 - accuracy: 0.9946\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 913us/step - loss: 0.0202 - accuracy: 0.9799\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.0150 - accuracy: 0.9946\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 576us/step - loss: 0.0152 - accuracy: 0.9917\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0109 - accuracy: 0.9970\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.0246 - accuracy: 0.9816\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.0157 - accuracy: 0.9904\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.0222 - accuracy: 0.9786\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.0186 - accuracy: 0.9815\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.0189 - accuracy: 0.9865\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.0120 - accuracy: 0.9962\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.0091 - accuracy: 0.9903\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0188 - accuracy: 0.9876\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0120 - accuracy: 0.9929\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.0095 - accuracy: 0.9915\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000271FBBAD670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1899 - accuracy: 0.7619\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.2455 - accuracy: 0.5591\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.2424 - accuracy: 0.5544\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2390 - accuracy: 0.5357\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.2287 - accuracy: 0.5780\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2182 - accuracy: 0.5803\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2142 - accuracy: 0.7643\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1928 - accuracy: 0.8086\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.1859 - accuracy: 0.7830\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.1755 - accuracy: 0.8141\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1750 - accuracy: 0.8078\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.1719 - accuracy: 0.7746\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.1586 - accuracy: 0.7891\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.1520 - accuracy: 0.8002\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.1602 - accuracy: 0.7950\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1497 - accuracy: 0.8144\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.1465 - accuracy: 0.8250\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.1404 - accuracy: 0.8044\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1205 - accuracy: 0.8705\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1244 - accuracy: 0.8037\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.1490 - accuracy: 0.7803\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1336 - accuracy: 0.8230\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1244 - accuracy: 0.8369\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.1260 - accuracy: 0.8497\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1064 - accuracy: 0.8658\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.1265 - accuracy: 0.8537\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1131 - accuracy: 0.8512\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1316 - accuracy: 0.8176\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.1051 - accuracy: 0.8581\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.1100 - accuracy: 0.9003\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.1201 - accuracy: 0.8434\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.1294 - accuracy: 0.8290\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.1119 - accuracy: 0.8335\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1034 - accuracy: 0.8903\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.1165 - accuracy: 0.8200\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.1065 - accuracy: 0.8440\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1085 - accuracy: 0.8749\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.0777 - accuracy: 0.9216\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.1111 - accuracy: 0.8517\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.0992 - accuracy: 0.9031\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.0953 - accuracy: 0.8933\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.0944 - accuracy: 0.8782\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.0929 - accuracy: 0.9064\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.0909 - accuracy: 0.9054\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.0907 - accuracy: 0.8930\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.0960 - accuracy: 0.9116\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0934 - accuracy: 0.9205\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.0792 - accuracy: 0.9230\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0793 - accuracy: 0.8810\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.0921 - accuracy: 0.9244\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.0871 - accuracy: 0.9053\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.0698 - accuracy: 0.9476\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.0743 - accuracy: 0.9248\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.0662 - accuracy: 0.9328\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.0731 - accuracy: 0.9081\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.0624 - accuracy: 0.9549\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.0886 - accuracy: 0.8896\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.0736 - accuracy: 0.9017\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0882 - accuracy: 0.9255\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.0810 - accuracy: 0.9175\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.0738 - accuracy: 0.8996\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.0600 - accuracy: 0.9587\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.0714 - accuracy: 0.9282\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.0614 - accuracy: 0.9583\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0561 - accuracy: 0.9349\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.0589 - accuracy: 0.9451\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.0554 - accuracy: 0.9598\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.0450 - accuracy: 0.9587\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.0606 - accuracy: 0.9516\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.0638 - accuracy: 0.9258\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.0565 - accuracy: 0.9461\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.0609 - accuracy: 0.9436\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.0728 - accuracy: 0.9212\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.0516 - accuracy: 0.9637\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.0400 - accuracy: 0.9719\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.0545 - accuracy: 0.9316\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.0559 - accuracy: 0.9496\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.0458 - accuracy: 0.9685\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.0488 - accuracy: 0.9536\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0435 - accuracy: 0.9538\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 891us/step - loss: 0.0396 - accuracy: 0.9774\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.0415 - accuracy: 0.9531\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.0363 - accuracy: 0.9694\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0394 - accuracy: 0.9784\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.0342 - accuracy: 0.9762\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.0365 - accuracy: 0.9552\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.0387 - accuracy: 0.9635\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.0411 - accuracy: 0.9617\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.0384 - accuracy: 0.9643\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.0426 - accuracy: 0.9669\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.0308 - accuracy: 0.9567\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0348 - accuracy: 0.9736\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.0388 - accuracy: 0.9610\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0323 - accuracy: 0.9535\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.0245 - accuracy: 0.9961\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.0274 - accuracy: 0.9900\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.0374 - accuracy: 0.9718\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.0244 - accuracy: 0.9886\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.0259 - accuracy: 0.9827\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.0284 - accuracy: 0.9619\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.0197 - accuracy: 0.9907\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000271FBB2FAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0770 - accuracy: 0.9524\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 1s 873us/step - loss: 0.2463 - accuracy: 0.5533\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2361 - accuracy: 0.6232\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2301 - accuracy: 0.6718\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.2136 - accuracy: 0.7077\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2028 - accuracy: 0.7013\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.1965 - accuracy: 0.7926\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.1938 - accuracy: 0.7465\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1733 - accuracy: 0.8175\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1705 - accuracy: 0.8224\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1557 - accuracy: 0.8692\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.1621 - accuracy: 0.7969\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.1382 - accuracy: 0.8717\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.1546 - accuracy: 0.8286\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1508 - accuracy: 0.7901\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1344 - accuracy: 0.8596\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1386 - accuracy: 0.8439\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.1335 - accuracy: 0.8509\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1253 - accuracy: 0.8485\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.1257 - accuracy: 0.8496\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.1302 - accuracy: 0.8251\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1300 - accuracy: 0.8197\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1171 - accuracy: 0.9123\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.1052 - accuracy: 0.9084\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.1156 - accuracy: 0.8380\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.1259 - accuracy: 0.8319\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1059 - accuracy: 0.8937\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1005 - accuracy: 0.8690\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.0919 - accuracy: 0.9201\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.0879 - accuracy: 0.9086\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1084 - accuracy: 0.9012\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.0890 - accuracy: 0.9142\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.0902 - accuracy: 0.8686\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.0878 - accuracy: 0.9024\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9482\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.0844 - accuracy: 0.8761\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.0743 - accuracy: 0.9228\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0740 - accuracy: 0.9277\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0991 - accuracy: 0.8902\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0772 - accuracy: 0.9225\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.0734 - accuracy: 0.8983\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.0605 - accuracy: 0.9159\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.0709 - accuracy: 0.9355\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.0872 - accuracy: 0.9077\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.0729 - accuracy: 0.8861\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0698 - accuracy: 0.9431\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.0688 - accuracy: 0.9141\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.0697 - accuracy: 0.9358\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.0588 - accuracy: 0.9455\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.0708 - accuracy: 0.9547\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0554 - accuracy: 0.9389\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0657 - accuracy: 0.9198\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 780us/step - loss: 0.0561 - accuracy: 0.9378\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.0647 - accuracy: 0.9297\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.0498 - accuracy: 0.9651\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.0509 - accuracy: 0.9537\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.0487 - accuracy: 0.9486\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.0494 - accuracy: 0.9419\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0489 - accuracy: 0.9299\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.0540 - accuracy: 0.9333\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.0552 - accuracy: 0.9326\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.0465 - accuracy: 0.9733\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.0431 - accuracy: 0.9429\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.0439 - accuracy: 0.9683\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.0398 - accuracy: 0.9619\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.0423 - accuracy: 0.9690\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.0366 - accuracy: 0.9760\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.0429 - accuracy: 0.9657\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.0461 - accuracy: 0.9630\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.0423 - accuracy: 0.9641\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.0349 - accuracy: 0.9562\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.0284 - accuracy: 0.9837\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0415 - accuracy: 0.9605\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.0307 - accuracy: 0.9866\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.0304 - accuracy: 0.9785\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.0381 - accuracy: 0.9689\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.0328 - accuracy: 0.9711\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0299 - accuracy: 0.9756\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0236 - accuracy: 0.9747\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.0216 - accuracy: 0.9878\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.0257 - accuracy: 0.9742\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.0322 - accuracy: 0.9814\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.0227 - accuracy: 0.9820\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.0262 - accuracy: 0.9867\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.0185 - accuracy: 0.9839\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.0359 - accuracy: 0.9709\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.0249 - accuracy: 0.9889\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.0264 - accuracy: 0.9853\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.0129 - accuracy: 0.9968\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.0315 - accuracy: 0.9785\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.0146 - accuracy: 0.9978\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.0198 - accuracy: 0.9844\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.0184 - accuracy: 0.9994\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.0160 - accuracy: 0.9930\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.0214 - accuracy: 0.9841\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.0239 - accuracy: 0.9791\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.0168 - accuracy: 0.9963\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.0206 - accuracy: 0.9882\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.0285 - accuracy: 0.9760\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.0230 - accuracy: 0.9860\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.0198 - accuracy: 0.9862\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000271FBB2FDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1179 - accuracy: 0.8571\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2476 - accuracy: 0.5626\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2395 - accuracy: 0.6587\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2258 - accuracy: 0.7349\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2126 - accuracy: 0.7042\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2006 - accuracy: 0.7537\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.1941 - accuracy: 0.7472\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.1900 - accuracy: 0.7208\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1701 - accuracy: 0.8300\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1739 - accuracy: 0.7938\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.1532 - accuracy: 0.8425\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.1571 - accuracy: 0.7639\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1475 - accuracy: 0.8113\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1485 - accuracy: 0.8294\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1508 - accuracy: 0.7544\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1331 - accuracy: 0.8659\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.1380 - accuracy: 0.8308\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1387 - accuracy: 0.8547\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1322 - accuracy: 0.8513\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1271 - accuracy: 0.8238\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1454 - accuracy: 0.7877\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.1628 - accuracy: 0.7514\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.1149 - accuracy: 0.8465\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1273 - accuracy: 0.8123\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 845us/step - loss: 0.1106 - accuracy: 0.8742\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1158 - accuracy: 0.8473\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1130 - accuracy: 0.8419\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1195 - accuracy: 0.8536\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1151 - accuracy: 0.8469\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.1197 - accuracy: 0.8292\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1224 - accuracy: 0.8003\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.1122 - accuracy: 0.8662\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.0908 - accuracy: 0.8979\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 1.00 - 0s 803us/step - loss: 0.0879 - accuracy: 0.8957\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.1017 - accuracy: 0.8718\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.0950 - accuracy: 0.8647\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0980 - accuracy: 0.8787\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.0855 - accuracy: 0.9033\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.1248 - accuracy: 0.8363\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.0929 - accuracy: 0.8909\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0960 - accuracy: 0.8450\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.0852 - accuracy: 0.8674\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0904 - accuracy: 0.9215\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.1070 - accuracy: 0.8575\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.0863 - accuracy: 0.8789\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.1044 - accuracy: 0.8845\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.0857 - accuracy: 0.9046\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1038 - accuracy: 0.8680\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0776 - accuracy: 0.9047\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.1104 - accuracy: 0.8392\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.0722 - accuracy: 0.9028\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0894 - accuracy: 0.8594\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0822 - accuracy: 0.8941\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.0795 - accuracy: 0.8925\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.0811 - accuracy: 0.8968\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.0650 - accuracy: 0.9268\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.0814 - accuracy: 0.9034\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.0687 - accuracy: 0.9167\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.0725 - accuracy: 0.9447\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.0709 - accuracy: 0.9160\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.0696 - accuracy: 0.9092\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.0768 - accuracy: 0.9086\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.0610 - accuracy: 0.9540\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.0577 - accuracy: 0.9530\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.0657 - accuracy: 0.9258\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.0747 - accuracy: 0.9066\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.0755 - accuracy: 0.9002\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.0499 - accuracy: 0.9566\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.0650 - accuracy: 0.9425\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.0526 - accuracy: 0.9484\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.0765 - accuracy: 0.9077\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.0650 - accuracy: 0.9213\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.0520 - accuracy: 0.9721\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.0511 - accuracy: 0.9508\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0505 - accuracy: 0.9612\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0643 - accuracy: 0.9076\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.0698 - accuracy: 0.9001\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.0459 - accuracy: 0.9637\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.0468 - accuracy: 0.9553\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0497 - accuracy: 0.9467\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0463 - accuracy: 0.9685\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.0600 - accuracy: 0.9095\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.0544 - accuracy: 0.9555\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.0460 - accuracy: 0.9687\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0356 - accuracy: 0.9804\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.0385 - accuracy: 0.9743\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.0505 - accuracy: 0.9484\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0407 - accuracy: 0.9560\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0376 - accuracy: 0.9830\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0375 - accuracy: 0.9592\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.0271 - accuracy: 0.9821\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.0388 - accuracy: 0.9631\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.0334 - accuracy: 0.9750\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0350 - accuracy: 0.9830\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.0321 - accuracy: 0.9727\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0344 - accuracy: 0.9837\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.0298 - accuracy: 0.9842\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.0297 - accuracy: 0.9832\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.0290 - accuracy: 0.9722\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.0277 - accuracy: 0.9845\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.0244 - accuracy: 0.9859\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000271FA4A6160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1534 - accuracy: 0.7143\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.2504 - accuracy: 0.4826\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.2411 - accuracy: 0.6401\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2366 - accuracy: 0.6164\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2269 - accuracy: 0.7085\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2201 - accuracy: 0.6788\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2172 - accuracy: 0.6782\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2088 - accuracy: 0.7084\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.1949 - accuracy: 0.7349\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.1855 - accuracy: 0.7758\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1783 - accuracy: 0.8077\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.1677 - accuracy: 0.7519\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.1736 - accuracy: 0.7443\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.1632 - accuracy: 0.7984\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.1526 - accuracy: 0.7567\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.1488 - accuracy: 0.8120\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.1468 - accuracy: 0.8213\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1452 - accuracy: 0.7988\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1364 - accuracy: 0.8276\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.1367 - accuracy: 0.8038\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1469 - accuracy: 0.7774\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1421 - accuracy: 0.8020\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.1301 - accuracy: 0.8263\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1314 - accuracy: 0.8689\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.1090 - accuracy: 0.8444\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.1292 - accuracy: 0.8268\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.1103 - accuracy: 0.8578\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1143 - accuracy: 0.8671\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1244 - accuracy: 0.7898\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 944us/step - loss: 0.1078 - accuracy: 0.8541\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 911us/step - loss: 0.1104 - accuracy: 0.9030\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.1198 - accuracy: 0.8076\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1125 - accuracy: 0.8360\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.0921 - accuracy: 0.9036\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.1006 - accuracy: 0.8678\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1117 - accuracy: 0.8096\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1138 - accuracy: 0.8605\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.0910 - accuracy: 0.9031\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1070 - accuracy: 0.8657\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.1022 - accuracy: 0.8869\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.1057 - accuracy: 0.8627\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.1039 - accuracy: 0.8461\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.1133 - accuracy: 0.8090\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1069 - accuracy: 0.8279\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.1030 - accuracy: 0.8646\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.0974 - accuracy: 0.8716\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.0923 - accuracy: 0.8964\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.8756\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.0800 - accuracy: 0.9052\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.1097 - accuracy: 0.8469\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 910us/step - loss: 0.1051 - accuracy: 0.8568\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.0903 - accuracy: 0.8927\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.0909 - accuracy: 0.8739\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.0913 - accuracy: 0.8908\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.0908 - accuracy: 0.8718\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.0753 - accuracy: 0.9155\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.0928 - accuracy: 0.8727\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0685 - accuracy: 0.9369\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.1006 - accuracy: 0.8554\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.0868 - accuracy: 0.8739\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.0823 - accuracy: 0.9043\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.0774 - accuracy: 0.8891\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0842 - accuracy: 0.8729\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.0621 - accuracy: 0.9345\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.0735 - accuracy: 0.9047\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.0782 - accuracy: 0.8847\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.0799 - accuracy: 0.8732\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.0519 - accuracy: 0.9321\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.0714 - accuracy: 0.9055\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.0651 - accuracy: 0.9315\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.0740 - accuracy: 0.8847\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0526 - accuracy: 0.9490\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.0586 - accuracy: 0.9402\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.0536 - accuracy: 0.9440\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.0508 - accuracy: 0.9606\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.0602 - accuracy: 0.9357\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.0725 - accuracy: 0.9363\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.0684 - accuracy: 0.9386\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.0566 - accuracy: 0.9505\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.0638 - accuracy: 0.9127\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 860us/step - loss: 0.0610 - accuracy: 0.9249\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.0576 - accuracy: 0.9502\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0565 - accuracy: 0.9269\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.0690 - accuracy: 0.9254\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0485 - accuracy: 0.9500\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.0394 - accuracy: 0.9743\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.0656 - accuracy: 0.9306\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.0566 - accuracy: 0.9502\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.0479 - accuracy: 0.9638\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.0593 - accuracy: 0.9500\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.0411 - accuracy: 0.9661\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.0377 - accuracy: 0.9649\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0512 - accuracy: 0.9558\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.0416 - accuracy: 0.9636\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.0411 - accuracy: 0.9555\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.0420 - accuracy: 0.9688\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.0503 - accuracy: 0.9497\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.0377 - accuracy: 0.9565\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.0426 - accuracy: 0.9609\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.0361 - accuracy: 0.9679\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.0354 - accuracy: 0.9613\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000271FCF323A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.1122 - accuracy: 0.8571\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2512 - accuracy: 0.4748\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2399 - accuracy: 0.5671\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2403 - accuracy: 0.6076\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.2316 - accuracy: 0.6834\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2201 - accuracy: 0.7073\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2180 - accuracy: 0.7163\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.2018 - accuracy: 0.7713\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.1874 - accuracy: 0.7603\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1762 - accuracy: 0.7752\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1715 - accuracy: 0.7563\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1564 - accuracy: 0.8289\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 1.00 - 0s 836us/step - loss: 0.1415 - accuracy: 0.8397\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1431 - accuracy: 0.8318\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1437 - accuracy: 0.8096\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.1357 - accuracy: 0.8074\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.1190 - accuracy: 0.8746\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.1159 - accuracy: 0.8412\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.1115 - accuracy: 0.8177\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 911us/step - loss: 0.1257 - accuracy: 0.8307\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.1277 - accuracy: 0.8140\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.1173 - accuracy: 0.8313\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.1048 - accuracy: 0.9039\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.0970 - accuracy: 0.8895\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0904 - accuracy: 0.8903\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1019 - accuracy: 0.8860\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0772 - accuracy: 0.9445\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0902 - accuracy: 0.9243\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.0764 - accuracy: 0.9059\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0934 - accuracy: 0.9130\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.0874 - accuracy: 0.9102\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.0979 - accuracy: 0.8661\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0892 - accuracy: 0.9339\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.0783 - accuracy: 0.9315\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.0713 - accuracy: 0.9392\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.0886 - accuracy: 0.8979\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.0776 - accuracy: 0.9303\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.0659 - accuracy: 0.9309\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.0657 - accuracy: 0.9042\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.0663 - accuracy: 0.9173\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.0634 - accuracy: 0.9363\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.0647 - accuracy: 0.9420\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.0565 - accuracy: 0.9573\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.0613 - accuracy: 0.9152\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0794 - accuracy: 0.8943\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.0597 - accuracy: 0.9360\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.0608 - accuracy: 0.9254\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.0536 - accuracy: 0.9560\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.0629 - accuracy: 0.9400\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.0653 - accuracy: 0.9190\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.0621 - accuracy: 0.9266\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0441 - accuracy: 0.9392\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 862us/step - loss: 0.0544 - accuracy: 0.9618\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.0438 - accuracy: 0.9662\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.0423 - accuracy: 0.9597\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.0394 - accuracy: 0.9592\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.0465 - accuracy: 0.9514\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0352 - accuracy: 0.9766\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.0465 - accuracy: 0.9637\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.0298 - accuracy: 0.9827\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.0407 - accuracy: 0.9642\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.0364 - accuracy: 0.9828\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.0419 - accuracy: 0.9620\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.0351 - accuracy: 0.9525\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.0296 - accuracy: 0.9789\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.0306 - accuracy: 0.9784\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0239 - accuracy: 0.9903\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0392 - accuracy: 0.9529\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.0354 - accuracy: 0.9743\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.0267 - accuracy: 0.9725\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.0397 - accuracy: 0.9498\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.0228 - accuracy: 0.9938\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.0247 - accuracy: 0.9782\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.0204 - accuracy: 0.9929\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.0204 - accuracy: 0.9800\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.0289 - accuracy: 0.9668\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.0241 - accuracy: 0.9802\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.0184 - accuracy: 0.9906\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.0169 - accuracy: 0.9978\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.0160 - accuracy: 0.9991\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.0213 - accuracy: 0.9907\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.0153 - accuracy: 0.9949\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0161 - accuracy: 0.9952\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.0169 - accuracy: 0.9991\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.0124 - accuracy: 0.9964\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.0113 - accuracy: 0.9966\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.0117 - accuracy: 0.9973\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.0141 - accuracy: 0.9991\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.0117 - accuracy: 0.9969\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.0079 - accuracy: 0.9966\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0087 - accuracy: 1.0000\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000271F8D4DEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1023 - accuracy: 0.8571\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2655 - accuracy: 0.4463\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2365 - accuracy: 0.5835\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2282 - accuracy: 0.6090\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2252 - accuracy: 0.6969\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.2137 - accuracy: 0.7119\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2091 - accuracy: 0.7182\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.1926 - accuracy: 0.7116\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.1905 - accuracy: 0.7719\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1744 - accuracy: 0.7711\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1695 - accuracy: 0.7681\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1701 - accuracy: 0.7393\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1586 - accuracy: 0.8358\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1509 - accuracy: 0.7837\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.1379 - accuracy: 0.8181\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1473 - accuracy: 0.8123\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.1420 - accuracy: 0.8414\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1297 - accuracy: 0.8427\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1479 - accuracy: 0.8109\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1354 - accuracy: 0.8221\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1327 - accuracy: 0.8417\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.1220 - accuracy: 0.8317\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1005 - accuracy: 0.8968\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1224 - accuracy: 0.8099\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 836us/step - loss: 0.1218 - accuracy: 0.8618\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1142 - accuracy: 0.8388\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1117 - accuracy: 0.8641\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.1053 - accuracy: 0.8905\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.80 - 0s 806us/step - loss: 0.1268 - accuracy: 0.8296\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1070 - accuracy: 0.8407\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1183 - accuracy: 0.8377\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.1138 - accuracy: 0.8476\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1198 - accuracy: 0.8391\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.1041 - accuracy: 0.8798\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1227 - accuracy: 0.8402\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.0987 - accuracy: 0.9012\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.0991 - accuracy: 0.8912\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.0879 - accuracy: 0.8861\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1100 - accuracy: 0.8802\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.0966 - accuracy: 0.9202\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1040 - accuracy: 0.8597\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0932 - accuracy: 0.8861\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.0966 - accuracy: 0.9300\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0932 - accuracy: 0.8894\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.0790 - accuracy: 0.9097\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0787 - accuracy: 0.9266\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.0734 - accuracy: 0.9251\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.0688 - accuracy: 0.9343\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.0846 - accuracy: 0.9090\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0936 - accuracy: 0.9117\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1048 - accuracy: 0.8820\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.0664 - accuracy: 0.9581\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.0662 - accuracy: 0.9412\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.0812 - accuracy: 0.8988\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.0671 - accuracy: 0.9315\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.0741 - accuracy: 0.9124\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.1022 - accuracy: 0.8856\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.0512 - accuracy: 0.9722\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.0604 - accuracy: 0.9452\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.0854 - accuracy: 0.9079\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.0677 - accuracy: 0.9327\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0653 - accuracy: 0.9264\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.0565 - accuracy: 0.9614\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.0575 - accuracy: 0.9419\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.0536 - accuracy: 0.9352\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.0588 - accuracy: 0.9301\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.0507 - accuracy: 0.9623\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.0558 - accuracy: 0.9523\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.0525 - accuracy: 0.9613\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.0454 - accuracy: 0.9621\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.0614 - accuracy: 0.9386\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.0581 - accuracy: 0.9449\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.0505 - accuracy: 0.9462\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0519 - accuracy: 0.9612\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.0380 - accuracy: 0.9791\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.0572 - accuracy: 0.9547\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.0414 - accuracy: 0.9750\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.0506 - accuracy: 0.9403\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.0325 - accuracy: 0.9740\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.0523 - accuracy: 0.9545\n",
      "Epoch 80/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0398 - accuracy: 0.9728\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.0512 - accuracy: 0.9531\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.0379 - accuracy: 0.9740\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.0323 - accuracy: 0.9836\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.0310 - accuracy: 0.9797\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.0262 - accuracy: 0.9810\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.0260 - accuracy: 0.9892\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0363 - accuracy: 0.9620\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.0351 - accuracy: 0.9668\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.0290 - accuracy: 0.9769\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.0187 - accuracy: 0.9932\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0252 - accuracy: 0.9880\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.0328 - accuracy: 0.9811\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.0252 - accuracy: 0.9873\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.0296 - accuracy: 0.9496\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.0219 - accuracy: 0.9856\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.0297 - accuracy: 0.9767\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.0193 - accuracy: 0.9955\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.0249 - accuracy: 0.9927\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.0220 - accuracy: 0.9897\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.0225 - accuracy: 0.9906\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000271FA70D670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0556 - accuracy: 0.9000\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.2496 - accuracy: 0.4770\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.2359 - accuracy: 0.5856\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2223 - accuracy: 0.7091\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.2164 - accuracy: 0.7382\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2008 - accuracy: 0.7531\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.1971 - accuracy: 0.7223\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.1802 - accuracy: 0.7858\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1891 - accuracy: 0.7714\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1720 - accuracy: 0.7928\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1684 - accuracy: 0.7647\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1725 - accuracy: 0.7336\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1629 - accuracy: 0.7855\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1532 - accuracy: 0.8089\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1342 - accuracy: 0.8516\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1595 - accuracy: 0.7469\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1381 - accuracy: 0.8371\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.1497 - accuracy: 0.7851\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.1325 - accuracy: 0.8339\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.1337 - accuracy: 0.7967\n",
      "Epoch 20/100\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.1560 - accuracy: 0.7725\n",
      "Epoch 21/100\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.1147 - accuracy: 0.8527\n",
      "Epoch 22/100\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1075 - accuracy: 0.8674\n",
      "Epoch 23/100\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1154 - accuracy: 0.8842\n",
      "Epoch 24/100\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1208 - accuracy: 0.8174\n",
      "Epoch 25/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.1041 - accuracy: 0.8653\n",
      "Epoch 26/100\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.1112 - accuracy: 0.8545\n",
      "Epoch 27/100\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.1051 - accuracy: 0.8710\n",
      "Epoch 28/100\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1137 - accuracy: 0.8473\n",
      "Epoch 29/100\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.1074 - accuracy: 0.8667\n",
      "Epoch 30/100\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.1110 - accuracy: 0.8779\n",
      "Epoch 31/100\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.1045 - accuracy: 0.8677\n",
      "Epoch 32/100\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1151 - accuracy: 0.8767\n",
      "Epoch 33/100\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.1070 - accuracy: 0.8554\n",
      "Epoch 34/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1057 - accuracy: 0.8793\n",
      "Epoch 35/100\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1054 - accuracy: 0.8795\n",
      "Epoch 36/100\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1091 - accuracy: 0.8607\n",
      "Epoch 37/100\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.0856 - accuracy: 0.9333\n",
      "Epoch 38/100\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.0936 - accuracy: 0.9078\n",
      "Epoch 39/100\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.0952 - accuracy: 0.8949\n",
      "Epoch 40/100\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.0811 - accuracy: 0.9155\n",
      "Epoch 41/100\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.0753 - accuracy: 0.9154\n",
      "Epoch 42/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.0629 - accuracy: 0.9382\n",
      "Epoch 43/100\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.0978 - accuracy: 0.8449\n",
      "Epoch 44/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.0757 - accuracy: 0.9147\n",
      "Epoch 45/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0911 - accuracy: 0.9052\n",
      "Epoch 46/100\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.0910 - accuracy: 0.9019\n",
      "Epoch 47/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.0743 - accuracy: 0.9226\n",
      "Epoch 48/100\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.0761 - accuracy: 0.9288\n",
      "Epoch 49/100\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.0882 - accuracy: 0.9111\n",
      "Epoch 50/100\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.0850 - accuracy: 0.9095\n",
      "Epoch 51/100\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.0895 - accuracy: 0.9199\n",
      "Epoch 52/100\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.0669 - accuracy: 0.9175\n",
      "Epoch 53/100\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.0720 - accuracy: 0.9463\n",
      "Epoch 54/100\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.0652 - accuracy: 0.9213\n",
      "Epoch 55/100\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.0627 - accuracy: 0.9316\n",
      "Epoch 56/100\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.0661 - accuracy: 0.9425\n",
      "Epoch 57/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.0594 - accuracy: 0.9197\n",
      "Epoch 58/100\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.0689 - accuracy: 0.9293\n",
      "Epoch 59/100\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.0532 - accuracy: 0.9269\n",
      "Epoch 60/100\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.0654 - accuracy: 0.9302\n",
      "Epoch 61/100\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.0535 - accuracy: 0.9632\n",
      "Epoch 62/100\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.0446 - accuracy: 0.9628\n",
      "Epoch 63/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0614 - accuracy: 0.9351\n",
      "Epoch 64/100\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.0571 - accuracy: 0.9388\n",
      "Epoch 65/100\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.0479 - accuracy: 0.9738\n",
      "Epoch 66/100\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.0529 - accuracy: 0.9594\n",
      "Epoch 67/100\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.0491 - accuracy: 0.9586\n",
      "Epoch 68/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0366 - accuracy: 0.9911\n",
      "Epoch 69/100\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.0474 - accuracy: 0.9668\n",
      "Epoch 70/100\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.0565 - accuracy: 0.9600\n",
      "Epoch 71/100\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.0435 - accuracy: 0.9786\n",
      "Epoch 72/100\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.0362 - accuracy: 0.9699\n",
      "Epoch 73/100\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.0353 - accuracy: 0.9875\n",
      "Epoch 74/100\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.0476 - accuracy: 0.9757\n",
      "Epoch 75/100\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.0368 - accuracy: 0.9774\n",
      "Epoch 76/100\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.0351 - accuracy: 0.9613\n",
      "Epoch 77/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.0374 - accuracy: 0.9780\n",
      "Epoch 78/100\n",
      "38/38 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.80 - 0s 809us/step - loss: 0.0379 - accuracy: 0.9764\n",
      "Epoch 79/100\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.0306 - accuracy: 0.9810\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 831us/step - loss: 0.0422 - accuracy: 0.9545\n",
      "Epoch 81/100\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.0325 - accuracy: 0.9761\n",
      "Epoch 82/100\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.0307 - accuracy: 0.9902\n",
      "Epoch 83/100\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.0283 - accuracy: 0.9881\n",
      "Epoch 84/100\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.0232 - accuracy: 0.9949\n",
      "Epoch 85/100\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.0283 - accuracy: 0.9763\n",
      "Epoch 86/100\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.0233 - accuracy: 0.9892\n",
      "Epoch 87/100\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.0333 - accuracy: 0.9779\n",
      "Epoch 88/100\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.0299 - accuracy: 0.9721\n",
      "Epoch 89/100\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.0294 - accuracy: 0.9830\n",
      "Epoch 90/100\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.0223 - accuracy: 0.9908\n",
      "Epoch 91/100\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.0344 - accuracy: 0.9833\n",
      "Epoch 92/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.0178 - accuracy: 0.9943\n",
      "Epoch 93/100\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.0268 - accuracy: 0.9869\n",
      "Epoch 94/100\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.0205 - accuracy: 0.9828\n",
      "Epoch 95/100\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.0215 - accuracy: 0.9784\n",
      "Epoch 96/100\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.0207 - accuracy: 0.9852\n",
      "Epoch 97/100\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.0244 - accuracy: 0.9889\n",
      "Epoch 98/100\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.0249 - accuracy: 0.9833\n",
      "Epoch 99/100\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.0244 - accuracy: 0.9852\n",
      "Epoch 100/100\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.0241 - accuracy: 0.9787\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000271FA2A1790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.1249 - accuracy: 0.8500\n",
      "\n",
      " 10 fold accuracy:  ['0.7143', '0.8571', '0.7619', '0.9524', '0.8571', '0.7143', '0.8571', '0.8571', '0.9000', '0.8500']\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ì„¤ì •, ì»´íŒŒì¼, ì‹¤í–‰í•˜ê¸°\n",
    "for train, test in skf.split(X,Y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim = 60, activation = 'relu'))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    model.fit(X[train], Y[train], epochs = 100, batch_size = 5)\n",
    "    k_accuracy = \"%.4f\" % (model.evaluate(X[test], Y[test])[1])\n",
    "    accuracy.append(k_accuracy)\n",
    "    \n",
    "# ê²°ê³¼ì¶œë ¥í•˜ê¸°\n",
    "print(\"\\n %.f fold accuracy: \" % n_fold, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3efb80b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
